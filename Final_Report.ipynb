{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Report.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendations Systems\n",
        "# Final Project Report - [Session-based recommendations with recurrent neural networks](https://arxiv.org/pdf/1511.06939)\n",
        "by   \n",
        "Gil Zeevi, 203909320  \n",
        "Gil Ayache, 200358612  \n",
        "**Group 25**"
      ],
      "metadata": {
        "id": "rkWvNPSTQ9e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Links\n",
        "- Paper: <a href='https://arxiv.org/pdf/1511.06939'>Article</a> <a href='https://github.com/hidasib/GRU4Rec'>Github</a>\n",
        "\n",
        "- Our Work - [Gil & Gil git repo](https://github.com/gilzeevi25/Session_based_recommendations)\n",
        "\n",
        "- References:<br>\n",
        "  * [Phạm Thanh Hùng (hungthanhpham94) repo](https://github.com/hungthanhpham94/GRU4REC-pytorch)\n",
        "  * [Younghun Song (yhs-968) repo](https://github.com/yhs968/pyGRU4REC)"
      ],
      "metadata": {
        "id": "7cWFJIuTWHvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets\n",
        "- <a href='https://www.kaggle.com/chadgostopp/recsys-challenge-2015?select=dataset-README.txt'>RecSys Challenge 2015</a>"
      ],
      "metadata": {
        "id": "pToSbrE4WpY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introduction** \n",
        "\n",
        "* What is the main objective of the paper, what are they trying to solve?\n",
        "\n",
        "    The problem of having\n",
        "    to base recommendations only on short session-based data instead of long user histories (as in the case of Netflix). In this situation\n",
        "    the frequently popular matrix factorization approaches become pretty unscalable, as we could witness 'with our bare hands' in this project where applying matrix factorization technique with BPR loss scaled awfully and didnt yield as great results as other baselines as ITEM-KNN and Session popularity model.\n",
        "    \n",
        "    This problem is usually overcome in practice by resorting to item-to-item recommendations,\n",
        "    i.e. recommending similar items.\n",
        "    The paper argues that by modeling the whole session,\n",
        "    more accurate recommendations can be provided.\n",
        "<br>\n",
        "\n",
        "* Evaluation - how are you going to evaluate performance?\n",
        "\n",
        "  We will use the same evaluation metrics as the paper did, both for baselines and GRU's, with another small, but important, addition:\n",
        "  1. <u>MRR@20</u> - The inverse of harmonic mean, which indicates the quality of the recommender system where it gives a score to in which positing the first relevant item occured.\n",
        "  2. <u>Recall@20</u> - can be simplfied as a Measurement of success in recommending. the fraction of how many item were actually correctly predicted.\n",
        "  3. <u>Time</u> -We will add a Training Time feature to each model in-order to compare running times.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f2YINw8RWtwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Anchor paper**\n",
        "\n",
        "1. State the anchor paper:\n",
        "[Session-based recommendations with recurrent neural networks](https://arxiv.org/pdf/1511.06939)\n",
        "\n",
        "2. Provide a short summary of the approach presented in the paper:\n",
        "\n",
        "  The ancor paper using the following improvement in order to overcome the problem of using only short session and the lack of user information in the seesion:\n",
        "\n",
        "* The model: GRU - it is a more elaborate model of an RNN unit that\n",
        "aims at dealing with the vanishing gradient problem.\n",
        "In general RNN makes predictions with data that comes in a form of a sequence.<br>\n",
        "\n",
        "     ![GRU](https://drive.google.com/uc?export=view&id=1KsNH1Hv1KTz5id-9p5bePKUnZoyVq3Ln)\n",
        "[taken from the session based with RNN paper](https://arxiv.org/pdf/1511.06939)\n",
        "\n",
        "* Session - parallel mini batches: The dataset which is feeded inside the GRU is first being reordered by sessions. Then the first event of the first X sessions, used to form an input of the first mini-batch which will feed the GRU as input. then, the second mini batch is formed from the second event of the active sessions and so on. if the session ends, the next available session is put in its place.watch the following image which demonstrates the process:\n",
        " ![pic](https://drive.google.com/uc?export=view&id=10TwEWnCpxwJZtJ3v8TWnijSk83f9jUtD)\n",
        "[taken from the session based with RNN paper](https://arxiv.org/pdf/1511.06939)\n",
        "\n",
        ": GRU - it is a more elaborate model of an RNN unit that\n",
        "aims at dealing with the vanishing gradient problem.\n",
        "In general RNN makes predictions with data that comes in a form of a sequence.\n",
        "\n",
        "* The loss functions: \n",
        "The paper use **pairwise ranking** loss instead of pointwise ranking loss, they did test the pointwise loss on cross-entropy that were unstable even with regularization.\n",
        "\n",
        "1. BPR - it optimizes a pairwise ranking loss, using Stochastic Gradient Descent.<br>\n",
        "To apply this method on sessions-based problems, the current state of the session is modeled as the average of the feature vectors of the items that have occurred in it so far.<br>\n",
        "the similarites of the feature vectors between a recommendable item and the items of the session so far are being averaged.\n",
        "  The loss which is being optimized is denoted as the following:\n",
        "\n",
        "\n",
        "$$L_{s}\\ \\ =\\ \\ -\\frac{1}{N_s} \\cdot \\sum_{j=1}^{N_s} log\\ ({\\large \\sigma}\\ (r_{s,i} - r_{s,j} )) $$\n",
        "\n",
        "\n",
        "\n",
        "$\\ \\ N_s \\ - Sample \\ Size $<br>\n",
        "$\\ \\ r_{s,i} \\ - Score \\ on \\ item \\ i\\ (or\\ negative\\ sampling\\ j\\ ) \\ at \\ the\\ given\\ point\\ of\\ session$<br>\n",
        "$\\ \\ {\\large \\sigma} \\ - Sigmoid\\ Function \\ \\frac{1}{1+e^{-x}}$\n",
        "\n",
        "\n",
        "2. TOP1 - The first part aims to push\n",
        "the target score above the score of the samples, while the second\n",
        "part lowers the score of negative samples towards zero. The latter\n",
        "acts as a regularizer, but instead of constraining the model weights\n",
        "directly, it penalizes high scores on the negative examples. Since\n",
        "all items act as a negative score in one training example or another,\n",
        "it generally pushes the scores down.\n",
        "\n",
        "$$L_{TOP1}\\ \\ =\\ \\ \\frac{1}{N_s} \\cdot \\sum_{j=1}^{N_s} {\\large \\sigma}\\ (r_{s,j} - r_{s,i} ) +{\\large \\sigma}( r_{s,j}^2)  $$\n",
        "<br><br>\n",
        "\n",
        "* Baselines results:\n",
        "\n",
        "     ![baseline](https://drive.google.com/uc?export=view&id=1ubWTQR9aE1NqH8SDaO1ZXY-sKDmkp_Xb)\n",
        "\n",
        "\n",
        "* Best evaluation metrics:\n",
        "\n",
        "     ![evaluation metrics](https://drive.google.com/uc?export=view&id=165V9o2TpaYfUPh_5YoyVJsFRmbWY2QPM)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d3GdMyMHW4k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Innovative part**\n",
        "\n",
        "* Choosing a smaller dataset consisting of 4.5 days but still outperforming the baselines with pretty low training time and siginficant higher score in RECALL@20\n",
        "\n",
        "* Showing training time comparisons between all models.\n",
        "\n",
        "* Presenting the Validation loss grpahs which were not presented at the original paper. its proves how all the GRU candidates doesnt overfit.\n",
        "\n",
        "* We questioned the paper's statement which claimed that GRU model with final activation of Tanh and Adagrad optimizer performs the best.\n",
        "we presented all sort of different model which performed almost as the paper's model, but with much less training time. our chosen model even outperformed the paper's model in terms of RECALL@20 and was 3 times faster in terms of training time.\n",
        "\n",
        "* Interactive notebook to see the affect of change in hyperparameters where all related work in github consists only python/Shell files as far as we've encourted."
      ],
      "metadata": {
        "id": "NbKeWCUQW-cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Summary of work and conclusion**\n",
        "\n",
        "* We worked at a different,smaller, scale as opposed to the GRU4REC paper, we did that in order to 'scale' the RNN network to our available limited resources (colab,local gpu & kaggle). this fact helped us examine a whole bunch of different GRU's with different optimizers and different losses, even though we couldnt afford applying the complete dataset.\n",
        "\n",
        "* We see that the basic Popularity model fails big time at big scales due to multiple items and too many relevant options.\n",
        "\n",
        "* We see how a slight change in POP model into per-session popularity model can still be really strong baseline model.\n",
        "Nevertheless we know from the original paper that as the dataset expands - popularity models performance will naturally go down\n",
        "\n",
        "* Most of our best picked GRU models (performed more than 500 parameters inspections) yielded around the same RECALL@20.\n",
        "We thus conclude that as opposed to what's presented in the paper, each GRU model can outperform all the baseline models presented.after careful hyperparameters tuning of course.\n",
        "\n",
        "* The chosen GRU model in paper had worse training time performance than our pick, even though it had slightly better MRR@20.\n",
        "The RECALL@20 was also in our favor and has beaten the paper model, at our chosen scale of course. Actually having a MRR@20 score of 0.22 or 0.24 doesnt really make significant difference in our opinion.\n",
        "\n",
        "* We managed to significantly outperform, in term of RECALL@20, the baselines with GRU. futhermore, we know that as the dataset grows bigger, expanding the GRU units even will increase its performance.\n",
        "\n",
        "* We were surprised by the poor performance of classical MF. it performed poorly on the dataset, compared to the cpu time it took to train. BPR-MF demand many iterations in order to perform well, and at growing and expanding datasets, it becomes not scalable.<br><br>"
      ],
      "metadata": {
        "id": "6x78Jok7XCqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Future work and inspections:**\n",
        "\n",
        "If we had more time, we would definitely want to inspect multi features session based recommendations.\n",
        "it can be in adding context or other features to see how the GRU performs. <a href='http://www.hidasi.eu/content/p_rnn_recsys16.pdf'>it is actually also a work of Balázs Hidasi</a> (the author of the original inspected paper)."
      ],
      "metadata": {
        "id": "LKf-gjCUdb4E"
      }
    }
  ]
}